{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('bank-full.csv',delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the features from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['age', 'job', 'marital', 'education', 'balance', 'housing', 'contact', \n",
    "                    'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y']]\n",
    "\n",
    "\n",
    "df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the missing values are presented in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1\n",
    "What is the most frequent observation (mode) for the column education?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education_mode = df[[\"education\"]].mode()\n",
    "df_education_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "Create the correlation matrix for the numerical features of your dataset. In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "\n",
    "What are the two features that have the biggest correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "df_numerical = df[['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']]\n",
    "df_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the correlation matrix for the numerical features\n",
    "df_numerical_corr=df_numerical.corr()\n",
    "df_numerical_corr\n",
    "\n",
    "# Finding the two features with the highest correlation\n",
    "max_corr_value = df_numerical_corr.abs().unstack().sort_values()\n",
    "# Exclude the correlation of the same features (diagonal)\n",
    "max_corr_value = max_corr_value[max_corr_value < 1]\n",
    "\n",
    "# Getting the two features with the highest correlation\n",
    "top_correlation = max_corr_value.idxmax(), max_corr_value.max()\n",
    "\n",
    "top_correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target encoding\n",
    "Now we want to encode the y variable.\n",
    "Let's replace the values yes/no with 1/0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.y = (df.y == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data\n",
    "Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
    "Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n",
    "Make sure that the target value y is not in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into 60% train, 20% validation, and 20% test\n",
    "# First, we split into train (80%) and temp (20%) with a random_state of 42 for reproducibility\n",
    "train_test_split (df, test_size=0.2,random_state=1)\n",
    "df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "len(df_train_full),len(df_test)\n",
    "# Now, we split the temp set into validation (20%) and train (60%)\n",
    "df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=42)\n",
    "len(df_train),len(df_test),len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "y_train = df_train.y.values\n",
    "y_val = df_val.y.values\n",
    "y_test = df_test.y.values\n",
    "#drop y\n",
    "#del df_train['y']\n",
    "#del df_val['y']\n",
    "#del df_test['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3\n",
    "Calculate the mutual information score between y and other categorical variables in the dataset. Use the training set only.\n",
    "Round the scores to 2 decimals using round(score, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "global_mean = df_train_full.y.mean()\n",
    "round(global_mean, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "married_mean = df_train_full[df_train_full.marital == 'married'].y.mean()\n",
    "print('marital == married:', round(married_mean, 2))\n",
    "\n",
    "notmarried_mean = df_train_full[df_train_full.marital == 'married'].y.mean()\n",
    "print('marital == no:  ', round(notmarried_mean, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notmarried_mean / global_mean\n",
    "\n",
    "married_mean / global_mean\n",
    "\n",
    "df_group = df_train_full.groupby(by=\"marital\").y.agg(['mean'])\n",
    "df_group['diff'] = df_group['mean'] - global_mean\n",
    "df_group['risk'] = df_group['mean'] / global_mean\n",
    "df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['job', 'marital', 'education',  'housing',\n",
    "                 'contact', 'month',  'poutcome', 'y']\n",
    "numerical = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous','y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for col in categorical:\n",
    "    df_group = df_train_full.groupby(by=col).y.agg(['mean'])\n",
    "    df_group['diff'] = df_group['mean'] - global_mean\n",
    "    df_group['risk'] = df_group['mean'] / global_mean\n",
    "    display(df_group)\n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def calculate_mi(series):\n",
    "    return mutual_info_score(series, df_train_full.y)\n",
    "\n",
    "df_mi = df_train_full[categorical].apply(calculate_mi)\n",
    "df_mi = df_mi.sort_values(ascending=False).to_frame(name='MI')\n",
    "\n",
    "\n",
    "display(df_mi.head())\n",
    "display(df_mi.tail())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mi(series):\n",
    "    return mutual_info_score(series, df_train_full.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full[numerical].corrwith(df_train_full.y).to_frame('correlation')\n",
    "df_train_full.groupby(by='y')[numerical].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "train_dict = df_train[categorical + numerical].to_dict(orient='records')\n",
    "train_dict[0]\n",
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(train_dict)\n",
    "X_train = dv.transform(train_dict)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel_buvawpg\\AppData\\Local\\Temp\\ipykernel_151644\\1817052294.py:1: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  val_dict = df_val[categorical + numerical].to_dict(orient='records')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dict = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "model.predict_proba(X_val)\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "y_pred\n",
    "y = y_pred > 0.5\n",
    "(y_val == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5\n",
    "Let's find the least useful feature using the feature elimination technique.\n",
    "Train a model with all these features (using the same parameters as in Q4).\n",
    "Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
    "Which of following feature has the smallest difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'fitmodel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[449], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_val, y_val_pred)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model with all features and record accuracy\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m original_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# List of features to test\u001b[39;00m\n\u001b[0;32m     21\u001b[0m features_to_test \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarital\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprevious\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[449], line 9\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(X_train, X_val, y_train, y_val)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_evaluate\u001b[39m(X_train, X_val, y_train, y_val):\n\u001b[0;32m      6\u001b[0m    \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train logistic regression model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     model \u001b[38;5;241m=\u001b[39m LogisticRegression(solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'fitmodel'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def train_and_evaluate(X_train, X_val, y_train, y_val):\n",
    "   \n",
    "# Train logistic regression model\n",
    "    model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    model.fitmodel.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation set and calculate accuracy\n",
    "y_val_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "\n",
    "\n",
    "# Train the model with all features and record accuracy\n",
    "original_accuracy = train_and_evaluate(X_train, X_val, y_train, y_val)\n",
    "\n",
    "# List of features to test\n",
    "features_to_test = ['age', 'balance', 'marital', 'previous']\n",
    "\n",
    "# Dictionary to store accuracy differences\n",
    "accuracy_differences = {}\n",
    "\n",
    "# Test excluding each feature\n",
    "for feature in features_to_test:\n",
    "    # Drop the feature from the dataset\n",
    "    X_train_excluded = X_train.drop(columns=[feature])\n",
    "    X_val_excluded = X_val.drop(columns=[feature])\n",
    "    \n",
    "    # Train and evaluate the model without the feature\n",
    "    accuracy_without_feature = train_and_evaluate(X_train_excluded, X_val_excluded, y_train, y_val)\n",
    "    \n",
    "    # Calculate the accuracy difference\n",
    "    accuracy_difference = round(original_accuracy - accuracy_without_feature, 4)\n",
    "    accuracy_differences[feature] = accuracy_difference\n",
    "\n",
    "# Output the feature with the smallest difference\n",
    "least_impactful_feature = min(accuracy_differences, key=accuracy_differences.get)\n",
    "\n",
    "print(f\"Feature with the smallest difference: {least_impactful_feature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonetl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
